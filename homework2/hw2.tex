\documentclass[12pt, fullpage,letterpaper]{article}\usepackage[margin=1in]{geometry}\usepackage{url}\usepackage{amsmath}\usepackage{amssymb}\usepackage{xspace}\usepackage{graphicx}\usepackage{hyperref}\usepackage{listings}\newcommand{\semester}{Spring 2020}\newcommand{\assignmentId}{2}\newcommand{\releaseDate}{11 Feb, 2019}\newcommand{\dueDate}{11:59pm, 25 Feb, 2019}\newcommand{\bx}{{\bf x}}\newcommand{\bw}{{\bf w}}\title{CS 5350/6350: Machine Learining \semester}\author{Homework \assignmentId}\date{Handed out: \releaseDate\\	Due: \dueDate}\title{CS 5350/6350: Machine Learning \semester}\author{Homework \assignmentId}\date{Handed out: \releaseDate\\  Due date: \dueDate}\author{Homework \assignmentId \\* Britton Gaul \\* u0915408}\begin{document}\maketitle\input{emacscomm}\section{Paper Problems [40 points + 8 bonus]}\begin{enumerate}\item~\begin{enumerate}	\item~	\begin{enumerate}		\item~		\newline According to Occam's Razor $H_2$ should be chosen for the result hypothesis. This is because both of the result hypothesises are consistent with the training data, but $H_2$ is smaller than $H$.		\item~		\newline This prinicple is reflected in the PAC guarantee, because the samller hypothesis that was chosen is more likely to be less then the total number of m examples. This is required because it is stated in the given equation. 	\end{enumerate}	\item~	\newline $3^{10} \cdot .90 = 53,145$ training examples needed. \end{enumerate}\item~\item~	\begin{enumerate}		\item~ 		\newline $x_1 - x_2-x_3 \geq 2 $		\item~		\newline $-x_1-x_2-x_3 \geq -1$		\item~		\newline $x_1+x_2+x_3+x_4 \geq 2$		\item ~		\newline $x_1+x_2-x_1+x_2 \geq 1$	\end{enumerate}				\item\item~  \begin{enumerate}	\item~	\newline $j(w, b)=\frac{1}{2} \sum_{n=1}^{m} (y_i - w^Tx_i)^2 \cdot b$ 	\item~	\item~	\item~  \end{enumerate}\end{enumerate}\section{Practice [60 points + 10 bonus]}\begin{enumerate}	\item~[2 Points]  	\newline Repository: https://github.com/BritGaul/CS5350\item~\begin{enumerate}	\item~	\newline \includegraphics{prob_2_2_a.png}	\newline When using the Adaboost decision trees the errors are smaller compared to a fully expanded decsion tree, which makes them more accurate. 		\item~	\newline Overall the bagged trees seem to be better than a single tree. However, when compared to the Addaboost trees it seems that the adaboosted trees are more accurate. 	\item~ 	\newline When comparing the results from the single tree learner verses the bagged trees, the bagged trees are more accurate. This is because the bagged trees can learn from the accuracies of previous trees, while the single tree has no other trees to learn from. 	 	\item~ 	\newline The random forrest algorithm runs more efficiently then the bagged trees alogorithm, but is less accurate then the bagged trees implementation. 	\end{enumerate}	\item~		\begin{enumerate}		\item~		\newline Learned weight vector: [0.92121924, 0.80795412, 0.87360641, 1.31402335, 0.13386879, 1.59860304, 1.01995499]		\newline Learning Rate: 0.0125		\newline \includegraphics{gradient_descent.png}		\item~		\newline Learned weight vector: [0.72954178 0.60583116, 0.65623542, 1.16051283, 0.10111029, 1.33361719, 0.8242626 ]		\newline Learning Rate: 0.001		\newline \includegraphics{stochastic_gradient.png}		\item~		\newline Learned weight vector: [0.92154947, 0.80829428, 0.87397433, 1.3142877,  0.13392374, 1.59904727, 1.02029192]		\newline The learned weight vector calcualted with an analytical form is very similar to the weight vector from the batch gradient descent method. By comparing all three weight vectors it seems that the stochastic gradient descent weight vector consistently returns the lowest weights. 		This could be because the other two methods will always return around the same numbers, where the stochastic approach could get lucky and find the most optimal result randomly. 	\end{enumerate}\end{enumerate}\end{document}%%% Local Variables:%%% mode: latex%%% TeX-master: t%%% End: